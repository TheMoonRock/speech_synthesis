{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d52593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import json\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_parse_xml_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    for sentence_idx, sentence in enumerate(root.findall('.//sentence')):\n",
    "        elements = list(sentence)\n",
    "        words_in_sentence = []\n",
    "        \n",
    "        # Собираем все слова и их позиции\n",
    "        word_elements = [(i, elem) for i, elem in enumerate(elements) if elem.tag == 'word']\n",
    "        \n",
    "        for word_pos, word_elem in word_elements:\n",
    "            original = word_elem.get('original', '')\n",
    "            has_stress = word_elem.get('nucleus') == '2'\n",
    "            \n",
    "            # Лингвистические характеристики\n",
    "            dictitem = word_elem.find('dictitem')\n",
    "            if dictitem is not None:\n",
    "                pos = dictitem.get('subpart_of_speech', '')\n",
    "                form = dictitem.get('form', '')\n",
    "                gender = dictitem.get('genesys', '')\n",
    "                semantics1 = dictitem.get('semantics1', '')\n",
    "                semantics2 = dictitem.get('semantics2', '')\n",
    "            else:\n",
    "                pos = form = gender = semantics1 = semantics2 = ''\n",
    "            \n",
    "            # Правильно определяем паузу после слова\n",
    "            pause_after = -1\n",
    "            # Ищем следующие элементы после текущего слова\n",
    "            for next_pos in range(word_pos + 1, len(elements)):\n",
    "                next_elem = elements[next_pos]\n",
    "                if next_elem.tag == 'pause':\n",
    "                    pause_time = next_elem.get('time')\n",
    "                    if pause_time and pause_time.isdigit():\n",
    "                        pause_after = int(pause_time)\n",
    "                    break\n",
    "                elif next_elem.tag == 'word':\n",
    "                    # Следующее слово - значит паузы нет\n",
    "                    break\n",
    "            \n",
    "            word_data = {\n",
    "                'sentence_id': sentence_idx,\n",
    "                'original': original,\n",
    "                'position_in_sentence': len(words_in_sentence),\n",
    "                'total_words_in_sentence': len(word_elements),\n",
    "                'words_before': len(words_in_sentence),\n",
    "                'words_after': len(word_elements) - len(words_in_sentence) - 1,\n",
    "                'has_capital': original and original[0].isupper(),\n",
    "                'word_length': len(original),\n",
    "                'part_of_speech': pos,\n",
    "                'form': form,\n",
    "                'gender': gender,\n",
    "                'semantics1': semantics1,\n",
    "                'semantics2': semantics2,\n",
    "                'phrasal_stress': has_stress,\n",
    "                'pause_length': pause_after\n",
    "            }\n",
    "            \n",
    "            words_in_sentence.append(word_data)\n",
    "        \n",
    "        sentences_data.extend(words_in_sentence)\n",
    "    \n",
    "    return pd.DataFrame(sentences_data)\n",
    "\n",
    "# Загружаем исправленные данные\n",
    "df_corrected = correct_parse_xml_data('gogol_utf8_cut.Result.xml')\n",
    "print(f\"Загружено {len(df_corrected)} слов\")\n",
    "print(f\"Уникальных предложений: {df_corrected['sentence_id'].nunique()}\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nФразовые ударения:\")\n",
    "print(df_corrected['phrasal_stress'].value_counts(normalize=True))\n",
    "\n",
    "pause_data = df_corrected[df_corrected['pause_length'] > 0]\n",
    "print(f\"\\nПаузы (только > 0):\")\n",
    "print(f\"Всего пауз: {len(pause_data)}\")\n",
    "if len(pause_data) > 0:\n",
    "    print(f\"Средняя длина: {pause_data['pause_length'].mean():.1f} мс\")\n",
    "    print(f\"Мин-Макс: {pause_data['pause_length'].min()}-{pause_data['pause_length'].max()} мс\")\n",
    "    print(f\"Примеры пауз: {pause_data['pause_length'].value_counts().head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1931714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ данных\n",
    "print(\"Распределение частей речи:\")\n",
    "print(df_corrected['part_of_speech'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nРаспределение форм слов:\")\n",
    "print(df_corrected['form'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nРаспределение родов:\")\n",
    "print(df_corrected['gender'].value_counts())\n",
    "\n",
    "# Посмотрим на примеры с паузами\n",
    "if len(pause_data) > 0:\n",
    "    print(\"\\nПримеры слов с паузами:\")\n",
    "    sample_pauses = pause_data[['original', 'pause_length', 'position_in_sentence', 'total_words_in_sentence']].head(10)\n",
    "    print(sample_pauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка данных\n",
    "print(\"Предобработка данных...\")\n",
    "\n",
    "# Заполняем пропуски\n",
    "df = df_corrected.copy()\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Кодируем категориальные переменные\n",
    "categorical_columns = ['part_of_speech', 'form', 'gender', 'semantics1', 'semantics2']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Признаки для моделей\n",
    "feature_columns = [\n",
    "    'position_in_sentence', \n",
    "    'total_words_in_sentence',\n",
    "    'words_before', \n",
    "    'words_after', \n",
    "    'has_capital', \n",
    "    'word_length'\n",
    "] + [col + '_encoded' for col in categorical_columns]\n",
    "\n",
    "print(f\"Используется {len(feature_columns)} признаков:\")\n",
    "print(feature_columns)\n",
    "\n",
    "# Подготовка данных для двух задач\n",
    "print(\"\\nПодготовка данных для двух задач...\")\n",
    "\n",
    "# Задача 1: Классификация фразового ударения\n",
    "X_stress = df[feature_columns]\n",
    "y_stress = df['phrasal_stress']\n",
    "\n",
    "# Задача 2: Регрессия для длины пауз (только слова с паузами)\n",
    "pause_data = df[df['pause_length'] > 0]\n",
    "X_pause = pause_data[feature_columns]\n",
    "y_pause = pause_data['pause_length']\n",
    "\n",
    "print(f\"Задача 1 (ударения): {X_stress.shape[0]} примеров\")\n",
    "print(f\"Задача 2 (паузы): {X_pause.shape[0]} примеров\")\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train_stress, X_test_stress, y_train_stress, y_test_stress = train_test_split(\n",
    "    X_stress, y_stress, test_size=0.2, random_state=42, stratify=y_stress\n",
    ")\n",
    "\n",
    "X_train_pause, X_test_pause, y_train_pause, y_test_pause = train_test_split(\n",
    "    X_pause, y_pause, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nРазделение данных:\")\n",
    "print(f\"Ударения - train: {X_train_stress.shape[0]}, test: {X_test_stress.shape[0]}\")\n",
    "print(f\"Паузы - train: {X_train_pause.shape[0]}, test: {X_test_pause.shape[0]}\")\n",
    "\n",
    "# Обучение моделей\n",
    "print(\"\\nОбучение моделей...\")\n",
    "\n",
    "# Модель для фразового ударения\n",
    "stress_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=100,\n",
    "    class_weights=[1, 3]  # Учитываем несбалансированность классов\n",
    ")\n",
    "\n",
    "print(\"Обучение модели для фразового ударения...\")\n",
    "stress_model.fit(\n",
    "    X_train_stress, y_train_stress,\n",
    "    eval_set=(X_test_stress, y_test_stress),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Модель для длины пауз\n",
    "pause_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\nОбучение модели для длины пауз...\")\n",
    "pause_model.fit(\n",
    "    X_train_pause, y_train_pause,\n",
    "    eval_set=(X_test_pause, y_test_pause),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Оценка моделей\n",
    "print(\"\\nОценка моделей...\")\n",
    "\n",
    "# Оценка классификации\n",
    "y_pred_stress = stress_model.predict(X_test_stress)\n",
    "print(\"Фразовое ударение - отчет классификации:\")\n",
    "print(classification_report(y_test_stress, y_pred_stress))\n",
    "\n",
    "# Оценка регрессии\n",
    "y_pred_pause = pause_model.predict(X_test_pause)\n",
    "mse = mean_squared_error(y_test_pause, y_pred_pause)\n",
    "mae = mean_absolute_error(y_test_pause, y_pred_pause)\n",
    "\n",
    "print(f\"\\nДлина пауз - метрики регрессии:\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f} мс\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.2f} мс\")\n",
    "\n",
    "# Важность признаков\n",
    "print(\"\\nВажность признаков для фразового ударения:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': stress_model.get_feature_importance()\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "print(\"\\nВажность признаков для длины пауз:\")\n",
    "feature_importance_pause = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': pause_model.get_feature_importance()\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance_pause.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для предсказания на новых данных и генерации JSON\n",
    "def predict_and_create_json(text_data, stress_model, pause_model, label_encoders):\n",
    "    \"\"\"\n",
    "    Предсказывает фразовые ударения и длины пауз для нового текста\n",
    "    и возвращает результат в требуемом JSON-формате\n",
    "    \"\"\"\n",
    "    # Здесь должна быть логика преобразования текста в признаки\n",
    "    # Для демонстрации используем существующие данные\n",
    "    \n",
    "    # Берем случайное предложение из тестовой выборки для демонстрации\n",
    "    sample_sentence = df[df['sentence_id'] == 0]  # Первое предложение\n",
    "    \n",
    "    result = []\n",
    "    sentence_data = {\"words\": []}\n",
    "    \n",
    "    for _, word_row in sample_sentence.iterrows():\n",
    "        # Подготавливаем признаки для предсказания\n",
    "        word_features = pd.DataFrame([word_row[feature_columns]])\n",
    "        \n",
    "        # Предсказываем ударение\n",
    "        stress_pred = stress_model.predict(word_features)[0]\n",
    "        \n",
    "        # Предсказываем длину паузы (только если модель обучена)\n",
    "        pause_pred = -1  # по умолчанию -1 (нет паузы)\n",
    "        if pause_model:\n",
    "            pause_pred = pause_model.predict(word_features)[0]\n",
    "            # Округляем до целых и убеждаемся, что не отрицательное\n",
    "            pause_pred = max(0, int(round(pause_pred)))\n",
    "        \n",
    "        # Добавляем слово в результат\n",
    "        sentence_data[\"words\"].append({\n",
    "            \"content\": word_row['original'],\n",
    "            \"phrasal_stress\": bool(stress_pred),\n",
    "            \"pause_len\": pause_pred\n",
    "        })\n",
    "    \n",
    "    result.append(sentence_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Генерируем пример JSON-результата\n",
    "print(\"\\nГенерация примера JSON-результата...\")\n",
    "json_example = predict_and_create_json(None, stress_model, pause_model, label_encoders)\n",
    "\n",
    "print(\"Пример JSON-структуры:\")\n",
    "print(json.dumps(json_example, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Сохраняем модели для будущего использования\n",
    "print(\"\\nСохранение моделей...\")\n",
    "stress_model.save_model('stress_model.cbm')\n",
    "pause_model.save_model('pause_model.cbm')\n",
    "\n",
    "print(\"Модели сохранены как 'stress_model.cbm' и 'pause_model.cbm'\")\n",
    "\n",
    "# Сохраняем кодировщики\n",
    "import joblib\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "print(\"Кодировщики сохранены как 'label_encoders.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5078a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем JSON в файл\n",
    "def save_json_result(json_data, filename='result.json'):\n",
    "    \"\"\"\n",
    "    Сохраняет JSON-результат в файл с правильным форматированием\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"JSON-результат сохранен в файл: {filename}\")\n",
    "\n",
    "# Сохраняем наш пример\n",
    "save_json_result(json_example, 'example_result.json')\n",
    "\n",
    "# Теперь создадим более реалистичный пример - обработаем несколько предложений\n",
    "def process_multiple_sentences(sentence_ids, stress_model, pause_model, label_encoders, df):\n",
    "    \"\"\"\n",
    "    Обрабатывает несколько предложений и возвращает JSON-результат\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for sentence_id in sentence_ids:\n",
    "        sentence_data = df[df['sentence_id'] == sentence_id]\n",
    "        \n",
    "        if len(sentence_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        sentence_result = {\"words\": []}\n",
    "        \n",
    "        for _, word_row in sentence_data.iterrows():\n",
    "            # Подготавливаем признаки для предсказания\n",
    "            word_features = pd.DataFrame([word_row[feature_columns]])\n",
    "            \n",
    "            # Предсказываем ударение\n",
    "            stress_pred = stress_model.predict(word_features)[0]\n",
    "            \n",
    "            # Предсказываем длину паузы\n",
    "            pause_pred = -1\n",
    "            if pause_model:\n",
    "                predicted_pause = pause_model.predict(word_features)[0]\n",
    "                # Округляем и проверяем, что не отрицательное\n",
    "                pause_pred = max(0, int(round(predicted_pause)))\n",
    "            \n",
    "            # Добавляем слово в результат\n",
    "            sentence_result[\"words\"].append({\n",
    "                \"content\": word_row['original'],\n",
    "                \"phrasal_stress\": bool(stress_pred),\n",
    "                \"pause_len\": pause_pred\n",
    "            })\n",
    "        \n",
    "        result.append(sentence_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Обрабатываем несколько предложений для демонстрации\n",
    "print(\"\\nОбработка нескольких предложений для демонстрации...\")\n",
    "sample_sentences = [0, 1, 2, 3]  # Первые 4 предложения\n",
    "multiple_results = process_multiple_sentences(sample_sentences, stress_model, pause_model, label_encoders, df)\n",
    "\n",
    "# Сохраняем результат с несколькими предложениями\n",
    "save_json_result(multiple_results, 'multiple_sentences_result.json')\n",
    "\n",
    "print(\"Содержимое файла multiple_sentences_result.json:\")\n",
    "print(json.dumps(multiple_results, indent=4, ensure_ascii=False)[:1000] + \"...\")  # Показываем начало\n",
    "\n",
    "# Также создадим функцию для обработки всего датасета (будет долго, но для полноты)\n",
    "def create_final_submission_json(stress_model, pause_model, label_encoders, df, output_file='final_submission.json'):\n",
    "    \"\"\"\n",
    "    Создает финальный JSON для submission на основе всех данных\n",
    "    В реальной ситуации здесь нужно обрабатывать тестовые данные\n",
    "    \"\"\"\n",
    "    print(f\"Создание финального submission файла...\")\n",
    "    \n",
    "    # Для демонстрации возьмем только первые 100 предложений\n",
    "    # В реальном задании нужно обработать всю тестовую выборку\n",
    "    demo_sentence_ids = df['sentence_id'].unique()[:100]\n",
    "    \n",
    "    result = process_multiple_sentences(demo_sentence_ids, stress_model, pause_model, label_encoders, df)\n",
    "    \n",
    "    # Сохраняем\n",
    "    save_json_result(result, output_file)\n",
    "    \n",
    "    print(f\"Файл {output_file} создан с {len(result)} предложениями\")\n",
    "    return result\n",
    "\n",
    "# Создаем демо-версию для submission\n",
    "final_result = create_final_submission_json(stress_model, pause_model, label_encoders, df, 'lab_submission.json')\n",
    "\n",
    "# Проверяем, что файлы создались\n",
    "import os\n",
    "print(f\"\\nСозданные файлы:\")\n",
    "for file in ['example_result.json', 'multiple_sentences_result.json', 'lab_submission.json']:\n",
    "    if os.path.exists(file):\n",
    "        file_size = os.path.getsize(file)\n",
    "        print(f\"✓ {file} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"✗ {file} - не найден\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
