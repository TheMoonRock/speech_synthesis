{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe75eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import json\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca3eb2",
   "metadata": {},
   "source": [
    "Этот код выполняет разбор XML-файла с речевыми данными и извлекает подробную информацию о каждом слове и его характеристиках.\n",
    "\n",
    "Вот что делает функция `correct_parse_xml_data` по шагам:\n",
    "\n",
    "1. Загружает и парсит XML-файл.\n",
    "2. Находит все теги `sentence` — каждое такое вложение рассматривается как предложение.\n",
    "3. Для каждого предложения перебирает все слова (`word`) и собирает их особенности:\n",
    "   - Исходный текст слова (`original`).\n",
    "   - Признак фразового ударения (`phrasal_stress`), если атрибут `nucleus` равен '2'.\n",
    "   - Лингвистические характеристики: часть речи, форма слова, род, семантика.\n",
    "4. Определяет длительность паузы после каждого слова (если пауза есть).\n",
    "5. Сохраняет информацию о позиции слова в предложении и общем количестве слов.\n",
    "6. Формирует список с данными для всех слов во всех предложениях и возвращает его в виде `DataFrame`.\n",
    "\n",
    "После этого код выводит статистику:\n",
    "\n",
    "- Общее число слов.\n",
    "- Число уникальных предложений.\n",
    "- Распределение фразовых ударений.\n",
    "- Данные по паузам — сколько пауз больше 0, средняя длина, диапазон и примеры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34de3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 75956 слов\n",
      "Уникальных предложений: 5001\n",
      "\n",
      "Фразовые ударения:\n",
      "phrasal_stress\n",
      "False    0.763837\n",
      "True     0.236163\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Паузы (только > 0):\n",
      "Всего пауз: 17962\n",
      "Средняя длина: 383.0 мс\n",
      "Мин-Макс: 1-1657 мс\n",
      "Примеры пауз: pause_length\n",
      "1020    1200\n",
      "800      310\n",
      "308      147\n",
      "297      113\n",
      "212       96\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def correct_parse_xml_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    for sentence_idx, sentence in enumerate(root.findall('.//sentence')):\n",
    "        elements = list(sentence)\n",
    "        words_in_sentence = []\n",
    "        \n",
    "        # Собираем все слова и их позиции\n",
    "        word_elements = [(i, elem) for i, elem in enumerate(elements) if elem.tag == 'word']\n",
    "        \n",
    "        for word_pos, word_elem in word_elements:\n",
    "            original = word_elem.get('original', '')\n",
    "            has_stress = word_elem.get('nucleus') == '2'\n",
    "            \n",
    "            # Лингвистические характеристики\n",
    "            dictitem = word_elem.find('dictitem')\n",
    "            if dictitem is not None:\n",
    "                pos = dictitem.get('subpart_of_speech', '')\n",
    "                form = dictitem.get('form', '')\n",
    "                gender = dictitem.get('genesys', '')\n",
    "                semantics1 = dictitem.get('semantics1', '')\n",
    "                semantics2 = dictitem.get('semantics2', '')\n",
    "            else:\n",
    "                pos = form = gender = semantics1 = semantics2 = ''\n",
    "            \n",
    "            # Правильно определяем паузу после слова\n",
    "            pause_after = -1\n",
    "            # Ищем следующие элементы после текущего слова\n",
    "            for next_pos in range(word_pos + 1, len(elements)):\n",
    "                next_elem = elements[next_pos]\n",
    "                if next_elem.tag == 'pause':\n",
    "                    pause_time = next_elem.get('time')\n",
    "                    if pause_time and pause_time.isdigit():\n",
    "                        pause_after = int(pause_time)\n",
    "                    break\n",
    "                elif next_elem.tag == 'word':\n",
    "                    # Следующее слово - значит паузы нет\n",
    "                    break\n",
    "            \n",
    "            word_data = {\n",
    "                'sentence_id': sentence_idx,\n",
    "                'original': original,\n",
    "                'position_in_sentence': len(words_in_sentence),\n",
    "                'total_words_in_sentence': len(word_elements),\n",
    "                'words_before': len(words_in_sentence),\n",
    "                'words_after': len(word_elements) - len(words_in_sentence) - 1,\n",
    "                'has_capital': original and original[0].isupper(),\n",
    "                'word_length': len(original),\n",
    "                'part_of_speech': pos,\n",
    "                'form': form,\n",
    "                'gender': gender,\n",
    "                'semantics1': semantics1,\n",
    "                'semantics2': semantics2,\n",
    "                'phrasal_stress': has_stress,\n",
    "                'pause_length': pause_after\n",
    "            }\n",
    "            \n",
    "            words_in_sentence.append(word_data)\n",
    "        \n",
    "        sentences_data.extend(words_in_sentence)\n",
    "    \n",
    "    return pd.DataFrame(sentences_data)\n",
    "\n",
    "# Загружаем исправленные данные\n",
    "df_corrected = correct_parse_xml_data('gogol_utf8_cut.Result.xml')\n",
    "print(f\"Загружено {len(df_corrected)} слов\")\n",
    "print(f\"Уникальных предложений: {df_corrected['sentence_id'].nunique()}\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nФразовые ударения:\")\n",
    "print(df_corrected['phrasal_stress'].value_counts(normalize=True))\n",
    "\n",
    "pause_data = df_corrected[df_corrected['pause_length'] > 0]\n",
    "print(f\"\\nПаузы (только > 0):\")\n",
    "print(f\"Всего пауз: {len(pause_data)}\")\n",
    "if len(pause_data) > 0:\n",
    "    print(f\"Средняя длина: {pause_data['pause_length'].mean():.1f} мс\")\n",
    "    print(f\"Мин-Макс: {pause_data['pause_length'].min()}-{pause_data['pause_length'].max()} мс\")\n",
    "    print(f\"Примеры пауз: {pause_data['pause_length'].value_counts().head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831ecd3",
   "metadata": {},
   "source": [
    "Этот код выполняет дополнительный анализ данных, извлечённых из XML, и выводит статистику по лингвистическим характеристикам слов, а также примеры слов с паузами.\n",
    "\n",
    "Подробно:\n",
    "\n",
    "- Выводит распределение 10 наиболее часто встречающихся частей речи (`part_of_speech`) в датасете.\n",
    "- Выводит распределение 10 наиболее частых форм слов (`form`).\n",
    "- Показывает распределение всех значений рода (`gender`).\n",
    "\n",
    "Далее, если в данных есть слова с паузами больше нуля (`pause_length > 0`):\n",
    "\n",
    "- Выводит первые 10 примеров слов с паузами, указывая сам текст слова (`original`), длину паузы, позицию слова в предложении и общее количество слов в этом предложении.\n",
    "\n",
    "Таким образом, код помогает быстро понять, какие части речи, формы и роды преобладают, а также посмотреть конкретные случаи с паузами для дальнейшего анализа или проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb4c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение частей речи:\n",
      "part_of_speech\n",
      "9     20128\n",
      "1     15593\n",
      "6     12607\n",
      "3      8872\n",
      "10     6897\n",
      "2      4495\n",
      "11     3201\n",
      "7      1309\n",
      "0      1069\n",
      "12      843\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Распределение форм слов:\n",
      "form\n",
      "0     27445\n",
      "1      7929\n",
      "2      3631\n",
      "5      3559\n",
      "67     3464\n",
      "60     2073\n",
      "6      1773\n",
      "9      1652\n",
      "20     1484\n",
      "10     1460\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Распределение родов:\n",
      "gender\n",
      "0    52108\n",
      "1     7019\n",
      "5     5053\n",
      "4     4256\n",
      "6     3860\n",
      "8     2023\n",
      "2     1410\n",
      "3      227\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Примеры слов с паузами:\n",
      "           original  ...  total_words_in_sentence\n",
      "1              души  ...                        2\n",
      "3            Гоголь  ...                        2\n",
      "5            ПЕРВЫЙ  ...                        2\n",
      "7            первая  ...                        2\n",
      "14          въехала  ...                       33\n",
      "19          бричка,  ...                       33\n",
      "23       холостяки:  ...                       33\n",
      "25   подполковники,  ...                       33\n",
      "26  штабс-капитаны,  ...                       33\n",
      "27        помещики,  ...                       33\n",
      "\n",
      "[10 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Анализ данных\n",
    "print(\"Распределение частей речи:\")\n",
    "print(df_corrected['part_of_speech'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nРаспределение форм слов:\")\n",
    "print(df_corrected['form'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nРаспределение родов:\")\n",
    "print(df_corrected['gender'].value_counts())\n",
    "\n",
    "# Посмотрим на примеры с паузами\n",
    "if len(pause_data) > 0:\n",
    "    print(\"\\nПримеры слов с паузами:\")\n",
    "    sample_pauses = pause_data[['original', 'pause_length', 'position_in_sentence', 'total_words_in_sentence']].head(10)\n",
    "    print(sample_pauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5b5a3",
   "metadata": {},
   "source": [
    "Этот код подготавливает данные из предыдущего анализа и обучает две модели машинного обучения для двух задач, затем оценивает их качество и выводит важность признаков.\n",
    "\n",
    "### Основные шаги:\n",
    "\n",
    "1. **Предобработка данных:**\n",
    "   - Создаёт копию исходных данных и заполняет пропуски пустыми строками.\n",
    "   - Кодирует категориальные переменные (`part_of_speech`, `form`, `gender`, `semantics1`, `semantics2`) с помощью `LabelEncoder` для использования в моделях.\n",
    "\n",
    "2. **Формирование признаков:**\n",
    "   - Определяет набор признаков, включающий позицию слова в предложении, длину слова, наличие заглавной буквы и закодированные категориальные признаки.\n",
    "\n",
    "3. **Подготовка данных для двух задач:**\n",
    "   - Задача 1 — классификация фразового ударения: признаковая матрица и целевая переменная с меткой ударения.\n",
    "   - Задача 2 — регрессия длины пауз: берутся только слова, после которых есть паузы, и готовятся соответствующие данные.\n",
    "\n",
    "4. **Разделение на обучающую и тестовую выборки:** по 80% на обучение и 20% на тестирование, с учётом баланса классов для задачи классификации.\n",
    "\n",
    "5. **Обучение моделей:**\n",
    "   - Для классификации ударений используется CatBoostClassifier с балансировкой классов.\n",
    "   - Для регрессии длины пауз — CatBoostRegressor.\n",
    "\n",
    "6. **Оценка качества:**\n",
    "   - Для классификации выводится отчёт с метриками (precision, recall, f1-score и др.).\n",
    "   - Для регрессии считаются MSE, MAE, RMSE — показатели точности предсказаний пауз.\n",
    "\n",
    "7. **Анализ важности признаков:**\n",
    "   - Выводятся топ-10 самых влияющих признаков для каждой из моделей.\n",
    "\n",
    "В целом, код строит модель, которая учится предсказывать, падает ли фразовое ударение на слово, а также модель, прогнозирующую длительность паузы после слова, используя лингвистические и позиционные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9163917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка данных...\n",
      "Используется 11 признаков:\n",
      "['position_in_sentence', 'total_words_in_sentence', 'words_before', 'words_after', 'has_capital', 'word_length', 'part_of_speech_encoded', 'form_encoded', 'gender_encoded', 'semantics1_encoded', 'semantics2_encoded']\n",
      "\n",
      "Подготовка данных для двух задач...\n",
      "Задача 1 (ударения): 75956 примеров\n",
      "Задача 2 (паузы): 17962 примеров\n",
      "\n",
      "Разделение данных:\n",
      "Ударения - train: 60764, test: 15192\n",
      "Паузы - train: 14369, test: 3593\n",
      "\n",
      "Обучение моделей...\n",
      "Обучение модели для фразового ударения...\n",
      "0:\tlearn: 0.6416621\ttest: 0.6424657\tbest: 0.6424657 (0)\ttotal: 69.5ms\tremaining: 1m 9s\n",
      "100:\tlearn: 0.4204364\ttest: 0.4316800\tbest: 0.4316800 (100)\ttotal: 444ms\tremaining: 3.95s\n",
      "200:\tlearn: 0.4018501\ttest: 0.4197831\tbest: 0.4197534 (197)\ttotal: 764ms\tremaining: 3.04s\n",
      "300:\tlearn: 0.3898546\ttest: 0.4149241\tbest: 0.4147905 (295)\ttotal: 1.09s\tremaining: 2.53s\n",
      "400:\tlearn: 0.3811757\ttest: 0.4129008\tbest: 0.4128290 (385)\ttotal: 1.47s\tremaining: 2.2s\n",
      "500:\tlearn: 0.3739343\ttest: 0.4127712\tbest: 0.4126019 (455)\ttotal: 1.84s\tremaining: 1.84s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4126019409\n",
      "bestIteration = 455\n",
      "\n",
      "Shrink model to first 456 iterations.\n",
      "\n",
      "Обучение модели для длины пауз...\n",
      "0:\tlearn: 236.4473749\ttest: 230.1887568\tbest: 230.1887568 (0)\ttotal: 2.32ms\tremaining: 2.31s\n",
      "100:\tlearn: 135.4933762\ttest: 135.5005138\tbest: 135.5005138 (100)\ttotal: 101ms\tremaining: 903ms\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 135.3315812\n",
      "bestIteration = 138\n",
      "\n",
      "Shrink model to first 139 iterations.\n",
      "\n",
      "Оценка моделей...\n",
      "Фразовое ударение - отчет классификации:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.82      0.87     11604\n",
      "        True       0.57      0.78      0.66      3588\n",
      "\n",
      "    accuracy                           0.81     15192\n",
      "   macro avg       0.75      0.80      0.76     15192\n",
      "weighted avg       0.84      0.81      0.82     15192\n",
      "\n",
      "\n",
      "Длина пауз - метрики регрессии:\n",
      "MSE: 18314.64\n",
      "MAE: 99.21 мс\n",
      "RMSE: 135.33 мс\n",
      "\n",
      "Важность признаков для фразового ударения:\n",
      "                    feature  importance\n",
      "5               word_length   34.222016\n",
      "3               words_after   21.091850\n",
      "7              form_encoded   10.663331\n",
      "8            gender_encoded    8.020518\n",
      "10       semantics2_encoded    6.964754\n",
      "6    part_of_speech_encoded    6.787325\n",
      "1   total_words_in_sentence    3.247240\n",
      "2              words_before    2.971315\n",
      "0      position_in_sentence    2.753059\n",
      "4               has_capital    1.640325\n",
      "\n",
      "Важность признаков для длины пауз:\n",
      "                   feature  importance\n",
      "3              words_after   92.413125\n",
      "1  total_words_in_sentence    1.357944\n",
      "7             form_encoded    1.106089\n",
      "5              word_length    1.031075\n",
      "2             words_before    0.846268\n",
      "0     position_in_sentence    0.827705\n",
      "6   part_of_speech_encoded    0.779852\n",
      "8           gender_encoded    0.469655\n",
      "9       semantics1_encoded    0.452138\n",
      "4              has_capital    0.431646\n"
     ]
    }
   ],
   "source": [
    "# Предобработка данных\n",
    "print(\"Предобработка данных...\")\n",
    "\n",
    "# Заполняем пропуски\n",
    "df = df_corrected.copy()\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Кодируем категориальные переменные\n",
    "categorical_columns = ['part_of_speech', 'form', 'gender', 'semantics1', 'semantics2']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Признаки для моделей\n",
    "feature_columns = [\n",
    "    'position_in_sentence', \n",
    "    'total_words_in_sentence',\n",
    "    'words_before', \n",
    "    'words_after', \n",
    "    'has_capital', \n",
    "    'word_length'\n",
    "] + [col + '_encoded' for col in categorical_columns]\n",
    "\n",
    "print(f\"Используется {len(feature_columns)} признаков:\")\n",
    "print(feature_columns)\n",
    "\n",
    "# Подготовка данных для двух задач\n",
    "print(\"\\nПодготовка данных для двух задач...\")\n",
    "\n",
    "# Задача 1: Классификация фразового ударения\n",
    "X_stress = df[feature_columns]\n",
    "y_stress = df['phrasal_stress']\n",
    "\n",
    "# Задача 2: Регрессия для длины пауз (только слова с паузами)\n",
    "pause_data = df[df['pause_length'] > 0]\n",
    "X_pause = pause_data[feature_columns]\n",
    "y_pause = pause_data['pause_length']\n",
    "\n",
    "print(f\"Задача 1 (ударения): {X_stress.shape[0]} примеров\")\n",
    "print(f\"Задача 2 (паузы): {X_pause.shape[0]} примеров\")\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train_stress, X_test_stress, y_train_stress, y_test_stress = train_test_split(\n",
    "    X_stress, y_stress, test_size=0.2, random_state=42, stratify=y_stress\n",
    ")\n",
    "\n",
    "X_train_pause, X_test_pause, y_train_pause, y_test_pause = train_test_split(\n",
    "    X_pause, y_pause, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nРазделение данных:\")\n",
    "print(f\"Ударения - train: {X_train_stress.shape[0]}, test: {X_test_stress.shape[0]}\")\n",
    "print(f\"Паузы - train: {X_train_pause.shape[0]}, test: {X_test_pause.shape[0]}\")\n",
    "\n",
    "# Обучение моделей\n",
    "print(\"\\nОбучение моделей...\")\n",
    "\n",
    "# Модель для фразового ударения\n",
    "stress_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=100,\n",
    "    class_weights=[1, 3]  # Учитываем несбалансированность классов\n",
    ")\n",
    "\n",
    "print(\"Обучение модели для фразового ударения...\")\n",
    "stress_model.fit(\n",
    "    X_train_stress, y_train_stress,\n",
    "    eval_set=(X_test_stress, y_test_stress),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Модель для длины пауз\n",
    "pause_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\nОбучение модели для длины пауз...\")\n",
    "pause_model.fit(\n",
    "    X_train_pause, y_train_pause,\n",
    "    eval_set=(X_test_pause, y_test_pause),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Оценка моделей\n",
    "print(\"\\nОценка моделей...\")\n",
    "\n",
    "# Оценка классификации\n",
    "y_pred_stress = stress_model.predict(X_test_stress)\n",
    "print(\"Фразовое ударение - отчет классификации:\")\n",
    "print(classification_report(y_test_stress, y_pred_stress))\n",
    "\n",
    "# Оценка регрессии\n",
    "y_pred_pause = pause_model.predict(X_test_pause)\n",
    "mse = mean_squared_error(y_test_pause, y_pred_pause)\n",
    "mae = mean_absolute_error(y_test_pause, y_pred_pause)\n",
    "\n",
    "print(f\"\\nДлина пауз - метрики регрессии:\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f} мс\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.2f} мс\")\n",
    "\n",
    "# Важность признаков\n",
    "print(\"\\nВажность признаков для фразового ударения:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': stress_model.get_feature_importance()\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "print(\"\\nВажность признаков для длины пауз:\")\n",
    "feature_importance_pause = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': pause_model.get_feature_importance()\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance_pause.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2515df",
   "metadata": {},
   "source": [
    "Этот код выполняет следующие действия:\n",
    "\n",
    "1. **Функция `predict_and_create_json`**:\n",
    "   - Принимает на вход новый текст (хотя здесь для демонстрации берёт данные из первого предложения набора данных).\n",
    "   - Для каждого слова в этом предложении подготовляет признаки.\n",
    "   - С помощью обученной модели классификации предсказывает, есть ли фразовое ударение.\n",
    "   - С помощью модели регрессии предсказывает длину паузы после слова (при наличии модели).\n",
    "   - Формирует результат в виде списка слов со своими характеристиками (`content`, `phrasal_stress`, `pause_len`) в удобном JSON-формате.\n",
    "\n",
    "2. **Демонстрация вывода JSON**:\n",
    "   - Создаёт пример JSON-результата с предсказаниями по первому предложению.\n",
    "   - Выводит этот JSON в консоль красиво отформатированным.\n",
    "\n",
    "3. **Сохранение моделей и кодировщиков**:\n",
    "   - Сохраняет обученные модели `stress_model` и `pause_model` в файлы для последующего использования.\n",
    "   - Сохраняет используемые `LabelEncoder`-ы в отдельный файл с помощью `joblib`.\n",
    "\n",
    "Таким образом, этот код готовит механизм, который получает текст, предсказывает ударения и паузы на уровне слов, возвращает результаты в JSON и даёт возможность сохранять подготовленные модели и кодировщики для практического применения в будущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189e8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация примера JSON-результата...\n",
      "Пример JSON-структуры:\n",
      "[\n",
      "    {\n",
      "        \"words\": [\n",
      "            {\n",
      "                \"content\": \"Мёртвые\",\n",
      "                \"phrasal_stress\": false,\n",
      "                \"pause_len\": 238\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"души\",\n",
      "                \"phrasal_stress\": true,\n",
      "                \"pause_len\": 650\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "\n",
      "Сохранение моделей...\n",
      "Модели сохранены как 'stress_model.cbm' и 'pause_model.cbm'\n",
      "Кодировщики сохранены как 'label_encoders.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Функция для предсказания на новых данных и генерации JSON\n",
    "def predict_and_create_json(text_data, stress_model, pause_model, label_encoders):\n",
    "    \"\"\"\n",
    "    Предсказывает фразовые ударения и длины пауз для нового текста\n",
    "    и возвращает результат в требуемом JSON-формате\n",
    "    \"\"\"\n",
    "    # Здесь должна быть логика преобразования текста в признаки\n",
    "    # Для демонстрации используем существующие данные\n",
    "    \n",
    "    # Берем случайное предложение из тестовой выборки для демонстрации\n",
    "    sample_sentence = df[df['sentence_id'] == 0]  # Первое предложение\n",
    "    \n",
    "    result = []\n",
    "    sentence_data = {\"words\": []}\n",
    "    \n",
    "    for _, word_row in sample_sentence.iterrows():\n",
    "        # Подготавливаем признаки для предсказания\n",
    "        word_features = pd.DataFrame([word_row[feature_columns]])\n",
    "        \n",
    "        # Предсказываем ударение\n",
    "        stress_pred = stress_model.predict(word_features)[0]\n",
    "        \n",
    "        # Предсказываем длину паузы (только если модель обучена)\n",
    "        pause_pred = -1  # по умолчанию -1 (нет паузы)\n",
    "        if pause_model:\n",
    "            pause_pred = pause_model.predict(word_features)[0]\n",
    "            # Округляем до целых и убеждаемся, что не отрицательное\n",
    "            pause_pred = max(0, int(round(pause_pred)))\n",
    "        \n",
    "        # Добавляем слово в результат\n",
    "        sentence_data[\"words\"].append({\n",
    "            \"content\": word_row['original'],\n",
    "            \"phrasal_stress\": bool(stress_pred),\n",
    "            \"pause_len\": pause_pred\n",
    "        })\n",
    "    \n",
    "    result.append(sentence_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Генерируем пример JSON-результата\n",
    "print(\"\\nГенерация примера JSON-результата...\")\n",
    "json_example = predict_and_create_json(None, stress_model, pause_model, label_encoders)\n",
    "\n",
    "print(\"Пример JSON-структуры:\")\n",
    "print(json.dumps(json_example, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Сохраняем модели для будущего использования\n",
    "print(\"\\nСохранение моделей...\")\n",
    "stress_model.save_model('stress_model.cbm')\n",
    "pause_model.save_model('pause_model.cbm')\n",
    "\n",
    "print(\"Модели сохранены как 'stress_model.cbm' и 'pause_model.cbm'\")\n",
    "\n",
    "# Сохраняем кодировщики\n",
    "import joblib\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "print(\"Кодировщики сохранены как 'label_encoders.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd68d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON-результат сохранен в файл: example_result.json\n",
      "\n",
      "Обработка нескольких предложений для демонстрации...\n",
      "JSON-результат сохранен в файл: multiple_sentences_result.json\n",
      "Содержимое файла multiple_sentences_result.json:\n",
      "[\n",
      "    {\n",
      "        \"words\": [\n",
      "            {\n",
      "                \"content\": \"Мёртвые\",\n",
      "                \"phrasal_stress\": false,\n",
      "                \"pause_len\": 238\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"души\",\n",
      "                \"phrasal_stress\": true,\n",
      "                \"pause_len\": 650\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"words\": [\n",
      "            {\n",
      "                \"content\": \"Николай\",\n",
      "                \"phrasal_stress\": false,\n",
      "                \"pause_len\": 160\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Гоголь\",\n",
      "                \"phrasal_stress\": true,\n",
      "                \"pause_len\": 719\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"words\": [\n",
      "            {\n",
      "                \"content\": \"ТОМ\",\n",
      "                \"phrasal_stress\": false,\n",
      "                \"pause_len\": 112\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"ПЕРВЫЙ\",\n",
      "                \"phrasal_stress\": true,\n",
      "                \"pause_len\": 974\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"words\": [\n",
      "            {\n",
      "  ...\n",
      "Создание финального submission файла...\n",
      "JSON-результат сохранен в файл: lab_submission.json\n",
      "Файл lab_submission.json создан с 100 предложениями\n",
      "\n",
      "Созданные файлы:\n",
      "✓ example_result.json (332 bytes)\n",
      "✓ multiple_sentences_result.json (1322 bytes)\n",
      "✓ lab_submission.json (347991 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем JSON в файл\n",
    "def save_json_result(json_data, filename='result.json'):\n",
    "    \"\"\"\n",
    "    Сохраняет JSON-результат в файл с правильным форматированием\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"JSON-результат сохранен в файл: {filename}\")\n",
    "\n",
    "# Сохраняем наш пример\n",
    "save_json_result(json_example, 'example_result.json')\n",
    "\n",
    "# Теперь создадим более реалистичный пример - обработаем несколько предложений\n",
    "def process_multiple_sentences(sentence_ids, stress_model, pause_model, label_encoders, df):\n",
    "    \"\"\"\n",
    "    Обрабатывает несколько предложений и возвращает JSON-результат\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for sentence_id in sentence_ids:\n",
    "        sentence_data = df[df['sentence_id'] == sentence_id]\n",
    "        \n",
    "        if len(sentence_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        sentence_result = {\"words\": []}\n",
    "        \n",
    "        for _, word_row in sentence_data.iterrows():\n",
    "            # Подготавливаем признаки для предсказания\n",
    "            word_features = pd.DataFrame([word_row[feature_columns]])\n",
    "            \n",
    "            # Предсказываем ударение\n",
    "            stress_pred = stress_model.predict(word_features)[0]\n",
    "            \n",
    "            # Предсказываем длину паузы\n",
    "            pause_pred = -1\n",
    "            if pause_model:\n",
    "                predicted_pause = pause_model.predict(word_features)[0]\n",
    "                # Округляем и проверяем, что не отрицательное\n",
    "                pause_pred = max(0, int(round(predicted_pause)))\n",
    "            \n",
    "            # Добавляем слово в результат\n",
    "            sentence_result[\"words\"].append({\n",
    "                \"content\": word_row['original'],\n",
    "                \"phrasal_stress\": bool(stress_pred),\n",
    "                \"pause_len\": pause_pred\n",
    "            })\n",
    "        \n",
    "        result.append(sentence_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Обрабатываем несколько предложений для демонстрации\n",
    "print(\"\\nОбработка нескольких предложений для демонстрации...\")\n",
    "sample_sentences = [0, 1, 2, 3]  # Первые 4 предложения\n",
    "multiple_results = process_multiple_sentences(sample_sentences, stress_model, pause_model, label_encoders, df)\n",
    "\n",
    "# Сохраняем результат с несколькими предложениями\n",
    "save_json_result(multiple_results, 'multiple_sentences_result.json')\n",
    "\n",
    "print(\"Содержимое файла multiple_sentences_result.json:\")\n",
    "print(json.dumps(multiple_results, indent=4, ensure_ascii=False)[:1000] + \"...\")  # Показываем начало\n",
    "\n",
    "# Также создадим функцию для обработки всего датасета (будет долго, но для полноты)\n",
    "def create_final_submission_json(stress_model, pause_model, label_encoders, df, output_file='final_submission.json'):\n",
    "    \"\"\"\n",
    "    Создает финальный JSON для submission на основе всех данных\n",
    "    В реальной ситуации здесь нужно обрабатывать тестовые данные\n",
    "    \"\"\"\n",
    "    print(f\"Создание финального submission файла...\")\n",
    "    \n",
    "    # Для демонстрации возьмем только первые 100 предложений\n",
    "    # В реальном задании нужно обработать всю тестовую выборку\n",
    "    demo_sentence_ids = df['sentence_id'].unique()[:100]\n",
    "    \n",
    "    result = process_multiple_sentences(demo_sentence_ids, stress_model, pause_model, label_encoders, df)\n",
    "    \n",
    "    # Сохраняем\n",
    "    save_json_result(result, output_file)\n",
    "    \n",
    "    print(f\"Файл {output_file} создан с {len(result)} предложениями\")\n",
    "    return result\n",
    "\n",
    "# Создаем демо-версию для submission\n",
    "final_result = create_final_submission_json(stress_model, pause_model, label_encoders, df, 'lab_submission.json')\n",
    "\n",
    "# Проверяем, что файлы создались\n",
    "import os\n",
    "print(f\"\\nСозданные файлы:\")\n",
    "for file in ['example_result.json', 'multiple_sentences_result.json', 'lab_submission.json']:\n",
    "    if os.path.exists(file):\n",
    "        file_size = os.path.getsize(file)\n",
    "        print(f\"✓ {file} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"✗ {file} - не найден\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292aa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем утилитный файл для использования моделей\n",
    "def create_prediction_script():\n",
    "    \"\"\"\n",
    "    Создает Python-скрипт для предсказания на новых данных\n",
    "    \"\"\"\n",
    "    script_content = '''\n",
    "import pandas as pd\n",
    "import json\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import joblib\n",
    "\n",
    "class ProsodyPredictor:\n",
    "    def __init__(self, stress_model_path, pause_model_path, encoders_path):\n",
    "        \"\"\"Загружает обученные модели и кодировщики\"\"\"\n",
    "        self.stress_model = CatBoostClassifier()\n",
    "        self.stress_model.load_model(stress_model_path)\n",
    "        \n",
    "        self.pause_model = CatBoostRegressor()\n",
    "        self.pause_model.load_model(pause_model_path)\n",
    "        \n",
    "        self.label_encoders = joblib.load(encoders_path)\n",
    "        \n",
    "        # Признаки, используемые в моделях\n",
    "        self.feature_columns = [\n",
    "            'position_in_sentence', 'total_words_in_sentence', 'words_before', \n",
    "            'words_after', 'has_capital', 'word_length', 'part_of_speech_encoded',\n",
    "            'form_encoded', 'gender_encoded', 'semantics1_encoded', 'semantics2_encoded'\n",
    "        ]\n",
    "    \n",
    "    def predict_sentence(self, sentence_features):\n",
    "        \"\"\"\n",
    "        Предсказывает ударения и паузы для одного предложения\n",
    "        \n",
    "        Args:\n",
    "            sentence_features: DataFrame с признаками для слов предложения\n",
    "        \n",
    "        Returns:\n",
    "            JSON-структура с результатами\n",
    "        \"\"\"\n",
    "        result = {\"words\": []}\n",
    "        \n",
    "        for _, word_row in sentence_features.iterrows():\n",
    "            # Подготавливаем признаки\n",
    "            word_features = pd.DataFrame([word_row[self.feature_columns]])\n",
    "            \n",
    "            # Предсказываем\n",
    "            stress_pred = self.stress_model.predict(word_features)[0]\n",
    "            pause_pred = self.pause_model.predict(word_features)[0]\n",
    "            pause_pred = max(0, int(round(pause_pred)))\n",
    "            \n",
    "            result[\"words\"].append({\n",
    "                \"content\": word_row['original'],\n",
    "                \"phrasal_stress\": bool(stress_pred),\n",
    "                \"pause_len\": pause_pred\n",
    "            })\n",
    "        \n",
    "        return [result]\n",
    "    \n",
    "    def save_predictions(self, predictions, filename):\n",
    "        \"\"\"Сохраняет предсказания в JSON-файл\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(predictions, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Предсказания сохранены в {filename}\")\n",
    "\n",
    "# Пример использования:\n",
    "if __name__ == \"__main__\":\n",
    "    # Загружаем модель\n",
    "    predictor = ProsodyPredictor(\n",
    "        stress_model_path='stress_model.cbm',\n",
    "        pause_model_path='pause_model.cbm', \n",
    "        encoders_path='label_encoders.pkl'\n",
    "    )\n",
    "    \n",
    "    # Здесь должен быть код для загрузки и предобработки тестовых данных\n",
    "    # Для демонстрации просто выводим сообщение\n",
    "    print(\"Модель загружена. Используйте predictor.predict_sentence() для предсказаний\")\n",
    "'''\n",
    "\n",
    "    with open('prosody_predictor.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(\"Создан файл prosody_predictor.py - утилита для предсказаний\")\n",
    "\n",
    "# Создаем утилитный скрипт\n",
    "create_prediction_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e7f95",
   "metadata": {},
   "source": [
    "1) пауза стоит после каждого слова. Это проблема. В разметке уже определены места пауз.\n",
    "Паузы ставить только в конце синтагмы. Тег intonation.\n",
    "2) Для каждой синтагмы найди ударное слово. Тег nucleus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
